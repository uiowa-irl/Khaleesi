{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import utilities\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = 'khaleesi/data/'\n",
    "db_dir = base_directory + 'crawl.sqlite'\n",
    "\n",
    "json_representation_dir = base_directory + 'crawl-js.json'\n",
    "json_representation_dir_with_responses = base_directory + 'crawl-js-with-responses.json'\n",
    "json_representation_dir_without_empty_response = base_directory + 'crawl-js-non-empty.json'\n",
    "json_representation_dir_connected = base_directory + 'crawl-js-connected.json'\n",
    "\n",
    "url_id_map_dir = base_directory + 'js-url-id-map.json'\n",
    "identifier_url_id_dir = base_directory + 'identifier-url-id-map.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect(db_dir)\n",
    "con.row_factory = sqlite3.Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying callstack for each request is slow in real time.\n",
    "#### We store the browser_id, visit_id, request_id to URL_id and URL_id to URL representations beforehand\n",
    "#### See next few cells to load the representation as json objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace browser_id with crawl_id for older schemas\n",
    "callstacks = pd.read_sql(\"SELECT browser_id, visit_id, request_id, call_stack FROM callstacks\", con, index_col=['browser_id', 'visit_id', 'request_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_2nd(string, substring):\n",
    "    return string.find(substring, string.find(substring) + 1)\n",
    "    \n",
    "def get_call_stack_url(current_stack):\n",
    "    call_stack = current_stack['call_stack'].iloc[0]\n",
    "    call_stack_items = call_stack.split('\\n')\n",
    "    \n",
    "    # unlikely\n",
    "    if len(call_stack_items) == 0:\n",
    "        return ''\n",
    "   \n",
    "    current_item = call_stack_items[0]\n",
    "    start = current_item.find('@') + 1\n",
    "    stop = find_2nd(current_item, ':')\n",
    "    \n",
    "    if current_item[start:stop].startswith('http'):\n",
    "        return current_item[start:stop]\n",
    "\n",
    "    if current_item.count('@') > 1:\n",
    "        start = find_2nd(current_item, '@') + 1\n",
    "        stop = find_2nd(current_item, ':')\n",
    "\n",
    "        if current_item[start:stop].startswith('http'):\n",
    "            return current_item[start:stop]\n",
    "\n",
    "            \n",
    "    for current_item in call_stack_items[1:]:\n",
    "        start = current_item.find('@') + 1\n",
    "        stop = find_2nd(current_item, ':')\n",
    "        \n",
    "        if current_item[start:stop].startswith('http'):\n",
    "            return current_item[start:stop]\n",
    "    \n",
    "    current_item = call_stack_items[0]\n",
    "    start = current_item.find('@') + 1\n",
    "    stop = find_2nd(current_item, ':')\n",
    "    return current_item[start:stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_ids = {}\n",
    "ids_URL = {}\n",
    "identifier_url_id = {}\n",
    "id_counter = 0\n",
    "\n",
    "pbar = tqdm(total=len(callstacks), position=0, leave=True)\n",
    "\n",
    "for index, current_stack in callstacks.groupby(level=[0,1,2]):\n",
    "    pbar.update(len(current_stack))\n",
    "\n",
    "    identifier = str(index[0]) + '|' + str(index[1]) + '|' + str(index[2])\n",
    "    executing_script_url = get_call_stack_url(current_stack)\n",
    "    \n",
    "    if executing_script_url == '':\n",
    "        continue\n",
    "        \n",
    "    if executing_script_url in URL_ids:\n",
    "        url_id = URL_ids[executing_script_url]\n",
    "        \n",
    "    else:\n",
    "        url_id = len(URL_ids) + 1\n",
    "        URL_ids[executing_script_url] = url_id\n",
    "        ids_URL[url_id] = executing_script_url\n",
    "        \n",
    "    identifier_url_id[identifier] = url_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.write_json(identifier_url_id_dir, identifier_url_id)\n",
    "utilities.write_json(url_id_map_dir, ids_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifier_url_id = utilities.read_json(identifier_url_id_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traverse requests table to create chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace browser_id with crawl_id for older schemas\n",
    "requests = pd.read_sql(\"SELECT browser_id, visit_id, request_id, url, top_level_url, referrer, headers, method, resource_type, time_stamp FROM http_requests\", con, index_col=['browser_id', 'visit_id', 'request_id'], parse_dates=['time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_representation = {}\n",
    "\n",
    "pbar = tqdm(total=len(requests), position=0, leave=True)\n",
    "\n",
    "for index, requests_chain in requests.groupby(level=[0,1,2]):\n",
    "    pbar.update(len(requests_chain))\n",
    "    \n",
    "    # we are ignoring HTTP chains here\n",
    "    if len(requests_chain) > 1:\n",
    "        continue\n",
    "    \n",
    "    crawl_browser_id = index[0]\n",
    "    visit_id = index[1]\n",
    "    request_id = index[2]\n",
    "\n",
    "    if str(index[0]) + '|' + str(index[1]) + '|' + str(index[2]) in identifier_url_id:\n",
    "        url_id = identifier_url_id[str(index[0]) + '|' + str(index[1]) + '|' + str(index[2])]\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "    \n",
    "    # change request_id to get a new id, according to script URL\n",
    "    chain_id = 'J|' + str(crawl_browser_id) + '|' + str(visit_id) + '|' + str(url_id)\n",
    "    \n",
    "    request = requests_chain.iloc[0]\n",
    "    chain_item = {}\n",
    "\n",
    "    chain_item['url'] = request['url']\n",
    "    chain_item['request_id'] = request_id\n",
    "    chain_item['referrer'] = request['referrer']\n",
    "    chain_item['request_headers'] = json.loads(request['headers'])\n",
    "    chain_item['resource_type'] = request['resource_type']\n",
    "    chain_item['method'] = request['method']\n",
    "    chain_item['time_stamp'] = request['time_stamp'].isoformat()\n",
    "\n",
    "        \n",
    "    if chain_id not in json_representation:\n",
    "        json_representation[chain_id] = {}\n",
    "        json_representation[chain_id]['top_url'] = request['top_level_url']\n",
    "        json_representation[chain_id]['content'] = []\n",
    "        json_representation[chain_id]['length'] = 0\n",
    "    \n",
    "    json_representation[chain_id]['content'].append(chain_item)\n",
    "    json_representation[chain_id]['length'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_json(json_representation):\n",
    "    for key in json_representation:\n",
    "        json_representation[key]['content'].sort(key=lambda k: parse(k['time_stamp']))\n",
    "        json_representation[key]['length'] = len(json_representation[key]['content'])\n",
    "        for idx, item in enumerate(json_representation[key]['content']):\n",
    "            item['redirect_id'] = idx\n",
    "    return json_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_representation = order_json(json_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.write_json(json_representation_dir, json_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read JSON here instead of processing the requests because it is already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_representation = utilities.read_json(json_representation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace browser_id with crawl_id for older schemas\n",
    "responses = pd.read_sql(\"SELECT browser_id, visit_id, request_id, url, response_status, headers, time_stamp FROM http_responses\", con, index_col=['browser_id', 'visit_id', 'request_id'], parse_dates=['time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_requests = set()\n",
    "for key in json_representation:\n",
    "    for idx, chain_item in enumerate(json_representation[key]['content']):\n",
    "        crawl_browser_id = str(key.split('|')[1])\n",
    "        visit_id = str(key.split('|')[2])\n",
    "        request_id = str(chain_item['request_id'])\n",
    "        js_requests.add(crawl_browser_id + '|' + visit_id + '|' + request_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(total=len(responses), position=0, leave=True)\n",
    "responses_data = {}\n",
    "\n",
    "for index, response in responses.iterrows():\n",
    "    pbar.update(1)\n",
    "\n",
    "    identifier = str(index[0]) + '|' + str(index[1]) + '|' + str(index[2])\n",
    "    \n",
    "    if identifier in js_requests and identifier not in responses_data:\n",
    "        responses_data[identifier] = {}\n",
    "\n",
    "        responses_data[identifier]['response_status'] = response['response_status']\n",
    "        responses_data[identifier]['response_headers'] = json.loads(response['headers'])\n",
    "        responses_data[identifier]['response_time_stamp'] = response['time_stamp'].isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_remove = {}\n",
    "less_than_2 = set()\n",
    "pbar = tqdm(total=len(json_representation), position=0, leave=True)\n",
    "\n",
    "for key in json_representation:\n",
    "    pbar.update(1)\n",
    "    if len(json_representation[key]['content']) <= 1: \n",
    "        less_than_2.add(key)\n",
    "        continue\n",
    "    \n",
    "    for idx, chain_item in enumerate(json_representation[key]['content']):\n",
    "        crawl_browser_id = str(key.split('|')[1])\n",
    "        visit_id = str(key.split('|')[2])\n",
    "        request_id = str(chain_item['request_id'])\n",
    "        \n",
    "        if crawl_browser_id + '|' + visit_id + '|' + request_id in responses_data:\n",
    "            response = responses_data[crawl_browser_id + '|' + visit_id + '|' + request_id]\n",
    "            chain_item['response_status'] = response['response_status']\n",
    "            chain_item['response_headers'] = response['response_headers']\n",
    "            chain_item['response_time_stamp'] = response['response_time_stamp']\n",
    "        else:\n",
    "            chain_item['response_status'] = 0\n",
    "            chain_item['response_headers'] = []\n",
    "            chain_item['response_time_stamp'] = ''\n",
    "            if key not in indexes_to_remove:\n",
    "                indexes_to_remove[key] = []\n",
    "            indexes_to_remove[key].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in less_than_2:\n",
    "    del json_representation[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.write_json(json_representation_dir_with_responses, json_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_2 = set()\n",
    "for key in indexes_to_remove:\n",
    "    content = [i for j, i in enumerate(json_representation[key]['content']) if j not in indexes_to_remove[key]]\n",
    "\n",
    "    json_representation[key]['content'] = content\n",
    "    if len(json_representation[key]['content']) <= 1: \n",
    "        less_than_2.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in less_than_2:\n",
    "    del json_representation[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.write_json(json_representation_dir_without_empty_response, json_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only keeping the connected ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_representation = utilities.read_json(json_representation_dir_without_empty_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib import parse as URLparse\n",
    "import base64\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identifier_cookies(cookie_string, cookie_length = 8):\n",
    "    cookie_set = set()\n",
    "    \n",
    "    for cookie in cookie_string.split('\\n'):\n",
    "        cookie = cookie.split(';')[0]\n",
    "        if cookie.count('=') >= 1:\n",
    "            cookie = cookie.split('=', 1)\n",
    "            cookie_set |= set(re.split('[^a-zA-Z0-9_=-]', cookie[1]))\n",
    "            cookie_set.add(cookie[0])\n",
    "        else:\n",
    "            cookie_set |= set(re.split('[^a-zA-Z0-9_=-]', cookie))\n",
    "    \n",
    "    # remove cookies with length < 8 \n",
    "    cookie_set = set([s for s in list(cookie_set) if len(s) >= cookie_length])\n",
    "    return cookie_set\n",
    "\n",
    "\n",
    "def get_identifiers_from_qs(url, qs_item_length = 8):\n",
    "    qs = URLparse.parse_qsl(URLparse.urlsplit(url).query)\n",
    "    qs_set = set()\n",
    "    \n",
    "    for item in qs:\n",
    "        qs_set |= set(re.split('[^a-zA-Z0-9_=-]', item[0]))\n",
    "        qs_set |= set(re.split('[^a-zA-Z0-9_=-]', item[1]))\n",
    "        \n",
    "    qs_set = set([s for s in list(qs_set) if len(s) >= qs_item_length])\n",
    "    return qs_set\n",
    "\n",
    "\n",
    "def get_identifiers_from_uncommon_headers(header_prop, item_length = 8):\n",
    "    splitted_header_prop_set = set()\n",
    "\n",
    "    splitted_header_prop = set(re.split('[^a-zA-Z0-9_=-]', header_prop))\n",
    "    splitted_header_prop_set = set([s for s in list(splitted_header_prop) if len(s) >= item_length])\n",
    "    return splitted_header_prop_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_csync_events(identifiers, next_url, next_identifiers):\n",
    "    for identifier in identifiers:    \n",
    "        if identifier in next_url or identifier in next_identifiers:\n",
    "            return True\n",
    "        \n",
    "        base64_identifier = base64.b64encode(identifier.encode('utf-8')).decode('utf8')\n",
    "        md5_identifier = hashlib.md5(identifier.encode('utf-8')).hexdigest()\n",
    "        sha1_identifier = hashlib.sha1(identifier.encode('utf-8')).hexdigest()\n",
    "        \n",
    "        if base64_identifier in next_url or base64_identifier in next_identifiers:\n",
    "            return True\n",
    "        elif md5_identifier in next_url or md5_identifier in next_identifiers:\n",
    "            return True\n",
    "        elif sha1_identifier in next_url or sha1_identifier in next_identifiers:\n",
    "            return True\n",
    "               \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non standard headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_http_headers = set()\n",
    "known_http_headers_raw = utilities.read_file_newline_stripped('khaleesi/data/common_headers.txt')\n",
    "for item in known_http_headers_raw:\n",
    "    if item.strip() != '':\n",
    "        known_http_headers.add(item.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes_to_remove = {}\n",
    "pbar = tqdm(total=len(json_representation), position=0, leave=True)\n",
    "\n",
    "for key in json_representation:\n",
    "    pbar.update(1)\n",
    "    indexes_to_remove[key] = []\n",
    "    for idx, item in enumerate(json_representation[key]['content']):\n",
    "        found = False\n",
    "        current_identifiers = set()\n",
    "        current_url = item['url']\n",
    "        current_referrer = item['referrer']\n",
    "        current_identifiers = set()\n",
    "        current_headers = item['request_headers']\n",
    "\n",
    "        sent_cookies = ''\n",
    "        for s_item in current_headers:\n",
    "            if s_item[0].lower() == 'cookie':\n",
    "                sent_cookies = s_item[1]\n",
    "            elif s_item[0].lower() not in known_http_headers:\n",
    "                current_identifiers |= get_identifiers_from_uncommon_headers(s_item[1])\n",
    "\n",
    "        recieved_cookies = ''\n",
    "        for s_item in item['response_headers']:\n",
    "            if s_item[0].lower() == 'set-cookie':\n",
    "                recieved_cookies = s_item[1]\n",
    "            elif s_item[0].lower() not in known_http_headers:\n",
    "                current_identifiers |= get_identifiers_from_uncommon_headers(s_item[1])\n",
    "\n",
    "        current_identifiers |= get_identifier_cookies(sent_cookies)\n",
    "        current_identifiers |= get_identifier_cookies(recieved_cookies)\n",
    "        current_identifiers |= get_identifiers_from_qs(current_url)\n",
    "        current_identifiers |= get_identifiers_from_qs(current_referrer)\n",
    "        \n",
    "        \n",
    "        # We need to traverse from the start. \n",
    "        # Becuase we never add the redirects who recieve the identifiers. \n",
    "        for idx_1, item_1 in enumerate(json_representation[key]['content']):\n",
    "            \n",
    "            if idx_1 == idx:\n",
    "                continue\n",
    "                \n",
    "            next_url = item_1['url']\n",
    "            next_headers = item_1['request_headers']\n",
    "\n",
    "            next_identifiers = set()\n",
    "            for s_item in next_headers:\n",
    "                if s_item[0].lower() == 'cookie':\n",
    "                    next_identifiers |= get_identifier_cookies(s_item[1])\n",
    "                elif s_item[0].lower() not in known_http_headers:\n",
    "                    next_identifiers |= get_identifiers_from_uncommon_headers(s_item[1])\n",
    "\n",
    "            if check_csync_events(current_identifiers, next_url, next_identifiers):\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "        if not found:\n",
    "            indexes_to_remove[key].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_2 = set()\n",
    "for key in indexes_to_remove:\n",
    "    content = [i for j, i in enumerate(json_representation[key]['content']) if j not in indexes_to_remove[key]]\n",
    "\n",
    "    json_representation[key]['content'] = content\n",
    "    if len(json_representation[key]['content']) <= 1: \n",
    "        less_than_2.add(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in less_than_2:\n",
    "    del json_representation[item]\n",
    "print(len(json_representation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.write_json(json_representation_dir_connected, json_representation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
